{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SumaiyaZohaRODELA/Evaluating-Vector-Databases-for-Image-Similarity-Search-Cultural-Pattern-Detection/blob/main/code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iV7rb6JUNp8I"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import random\n",
        "import pathlib\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsP820RZN7iV",
        "outputId": "bc9d3306-c616-48e5-87ac-014c99236010"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No GPU detected\n",
            "No GPUs available\n"
          ]
        }
      ],
      "source": [
        "# Check for GPU availability\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "if len(physical_devices) > 0:\n",
        "    print('GPU is available')\n",
        "else:\n",
        "    print('No GPU detected')\n",
        "\n",
        "num_gpus = len(physical_devices)\n",
        "\n",
        "if num_gpus > 0:\n",
        "    print(f\"Number of available GPUs: {num_gpus}\")\n",
        "    for i in range(num_gpus):\n",
        "        print(f\"GPU {i}: {tf.config.experimental.get_device_details(physical_devices[0])}\")\n",
        "else:\n",
        "    print(\"No GPUs available\")\n",
        "\n",
        "device = tf.device('gpu:0' if len(physical_devices) > 0 else 'cpu:0')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41o-V6E4MNQ3",
        "outputId": "89987491-3f93-4c11-ec73-8933907f740e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4FE8P9gAMQiW",
        "outputId": "e97e5987-71ce-4f29-a902-7779ceb4bbed"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['12.jpeg',\n",
              " '84.jpeg',\n",
              " '29.jpeg',\n",
              " '112.jpeg',\n",
              " '129.jpeg',\n",
              " '201.jpeg',\n",
              " '301.jpeg',\n",
              " '433.jpeg',\n",
              " '1774.jpeg',\n",
              " '1759.jpeg',\n",
              " '1746.jpeg',\n",
              " '1790.jpeg',\n",
              " '1813.jpeg',\n",
              " '2564.jpeg',\n",
              " '2636.jpeg',\n",
              " '2847.jpeg',\n",
              " '0.jpeg',\n",
              " '3723.jpeg',\n",
              " '4347.jpeg',\n",
              " '4323.jpeg',\n",
              " '4330.jpeg',\n",
              " '4315.jpeg',\n",
              " '4588.jpeg',\n",
              " '4626.jpeg',\n",
              " '4590.jpeg',\n",
              " '4625.jpeg',\n",
              " '7045.jpeg',\n",
              " '7037.jpeg',\n",
              " '7054.jpeg',\n",
              " '7061.jpeg',\n",
              " '40.jpeg',\n",
              " '144.jpeg',\n",
              " '156.jpeg',\n",
              " '173.jpeg',\n",
              " '229.jpeg',\n",
              " '316.jpeg',\n",
              " '289.jpeg',\n",
              " '478.jpeg',\n",
              " '477.jpeg',\n",
              " '490.jpeg',\n",
              " '1758.jpeg',\n",
              " '1775.jpeg',\n",
              " '1883.jpeg',\n",
              " '1855.jpeg',\n",
              " '1867.jpeg',\n",
              " '1800.jpeg',\n",
              " '2658.jpeg',\n",
              " '2565.jpeg',\n",
              " '2596.jpeg',\n",
              " '2694.jpeg',\n",
              " '3165.jpeg',\n",
              " '3164.jpeg',\n",
              " '3200.jpeg',\n",
              " '3734.jpeg',\n",
              " '4355.jpeg',\n",
              " '4540.jpeg',\n",
              " '4589.jpeg',\n",
              " '7038.jpeg',\n",
              " '7053.jpeg',\n",
              " '13.jpeg',\n",
              " '28.jpeg',\n",
              " '85.jpeg',\n",
              " '261.jpeg',\n",
              " '328.jpeg',\n",
              " '345.jpeg',\n",
              " '344.jpeg',\n",
              " '432.jpeg',\n",
              " '462.jpeg',\n",
              " '1773.jpeg',\n",
              " '1818.jpeg',\n",
              " '1899.jpeg',\n",
              " '2539.jpeg',\n",
              " '3238.jpeg',\n",
              " '3691.jpeg',\n",
              " '3707.jpeg',\n",
              " '3766.jpeg',\n",
              " '4354.jpeg',\n",
              " '4534.jpeg',\n",
              " '4604.jpeg',\n",
              " '57.jpeg',\n",
              " '117.jpeg',\n",
              " '244.jpeg',\n",
              " '256.jpeg',\n",
              " '300.jpeg',\n",
              " '333.jpeg',\n",
              " '418.jpeg',\n",
              " '444.jpeg',\n",
              " '1763.jpeg',\n",
              " '1812.jpeg',\n",
              " '1785.jpeg',\n",
              " '1827.jpeg',\n",
              " '2600.jpeg',\n",
              " '2601.jpeg',\n",
              " '2668.jpeg',\n",
              " '2695.jpeg',\n",
              " '3272.jpeg',\n",
              " '3755.jpeg',\n",
              " '4573.jpeg',\n",
              " '4602.jpeg',\n",
              " '7046.jpeg',\n",
              " '1.jpeg',\n",
              " '200.jpeg',\n",
              " '417.jpeg',\n",
              " '461.jpeg',\n",
              " '489.jpeg',\n",
              " '1844.jpeg',\n",
              " '1802.jpeg',\n",
              " '2659.jpeg',\n",
              " '3237.jpeg',\n",
              " '3739.jpeg',\n",
              " '4338.jpeg',\n",
              " '4564.jpeg',\n",
              " '4574.jpeg',\n",
              " '4614.jpeg',\n",
              " '7062.jpeg',\n",
              " '100.jpeg',\n",
              " '128.jpeg',\n",
              " '288.jpeg',\n",
              " '216.jpeg',\n",
              " '217.jpeg',\n",
              " '228.jpeg',\n",
              " '273.jpeg',\n",
              " '317.jpeg',\n",
              " '1840.jpeg',\n",
              " '1801.jpeg',\n",
              " '1882.jpeg',\n",
              " '2529.jpeg',\n",
              " '2574.jpeg',\n",
              " '2623.jpeg',\n",
              " '2693.jpeg',\n",
              " '2845.jpeg',\n",
              " '3193.jpeg',\n",
              " '3686.jpeg',\n",
              " '3702.jpeg',\n",
              " '3675.jpeg',\n",
              " '4314.jpeg',\n",
              " '4322.jpeg',\n",
              " '45.jpeg',\n",
              " '72.jpeg',\n",
              " '73.jpeg',\n",
              " '56.jpeg',\n",
              " '157.jpeg',\n",
              " '145.jpeg',\n",
              " '172.jpeg',\n",
              " '101.jpeg',\n",
              " '245.jpeg',\n",
              " '189.jpeg',\n",
              " '272.jpeg',\n",
              " '460.jpeg',\n",
              " '472.jpeg',\n",
              " '1747.jpeg',\n",
              " '1830.jpeg',\n",
              " '1884.jpeg',\n",
              " '1856.jpeg',\n",
              " '1872.jpeg',\n",
              " '1829.jpeg',\n",
              " '2632.jpeg',\n",
              " '3771.jpeg',\n",
              " '3750.jpeg',\n",
              " '4346.jpeg',\n",
              " '4339.jpeg',\n",
              " '4533.jpeg',\n",
              " '4603.jpeg',\n",
              " '4618.jpeg',\n",
              " '4613.jpeg',\n",
              " '7029.jpeg',\n",
              " '7030.jpeg',\n",
              " '7070.jpeg',\n",
              " '184.jpeg',\n",
              " '416.jpeg',\n",
              " '450.jpeg',\n",
              " '488.jpeg',\n",
              " '445.jpeg',\n",
              " '434.jpeg',\n",
              " '1786.jpeg',\n",
              " '1871.jpeg',\n",
              " '1845.jpeg',\n",
              " '1857.jpeg',\n",
              " '1748.jpeg',\n",
              " '1894.jpeg',\n",
              " '1898.jpeg',\n",
              " '2538.jpeg',\n",
              " '2846.jpeg',\n",
              " '3718.jpg',\n",
              " '4331.jpeg',\n",
              " '4624.jpeg',\n",
              " '7069.jpeg',\n",
              " '14075060472_974ee0b00b_b.jpg',\n",
              " '403849179-800x600.jpeg',\n",
              " '403849178-800x600.jpeg',\n",
              " '8572987636_9a2c56c0be.jpg',\n",
              " 'AJ01-1.jpg',\n",
              " 'Ae8d73fe0a28c4d5da548f4944e5c015eS.jpg_300x300.jpg',\n",
              " 'Ajrak-Frock-CU2--e25dd3b-behbud-boutique.jpg',\n",
              " 'Ajrak-lawn-dress.jpg',\n",
              " 'Ajrak-kurti-For-Baby-Girl-2-scaled.jpg',\n",
              " 'Ajrak20Shawl20Blue-900x900.jpg',\n",
              " 'Ajrak_Sindhi_3_grande.jpg',\n",
              " 'BlackAjrakFrockFront.jpg',\n",
              " 'BAJK__w1_all_set223_girldress_front__2020-11-13-19-6-50__1365X2048.jpg',\n",
              " 'BNBK-13.jpeg',\n",
              " 'FB-IMG_1695759841399_554x.jpg',\n",
              " 'FB_IMG_1569339504704.jpg',\n",
              " 'az-large-3165718.jpg',\n",
              " 'ajrak_fe0680fc-7b89-42e8-a89a-78e9db7417cc.jpg',\n",
              " 'cultural-photos-290.jpg',\n",
              " 'e7584fc0f8336678e17441dc4c227806.jpg',\n",
              " 'hq2.jpg',\n",
              " 'hq720.jpg',\n",
              " 'fd462773cc76d63f01d5515fd060f2ae.jpg',\n",
              " 'il_570xN.5381670830_o5st.jpg',\n",
              " 'image10.jpeg',\n",
              " 'il_fullxfull.3205221479_c80i.jpg',\n",
              " 'image25.jpeg',\n",
              " 'images102.jpg',\n",
              " 'image9.jpeg',\n",
              " 'images106.jpg',\n",
              " 'images118.jpg',\n",
              " 'images126.jpg',\n",
              " 'images132.jpg',\n",
              " 'images134.jpg',\n",
              " 'images133.jpg',\n",
              " 'images140.jpg',\n",
              " 'images141.jpg',\n",
              " 'images142.jpg',\n",
              " 'images14.jpg',\n",
              " 'images18.jpg',\n",
              " 'images2.jpg',\n",
              " 'images42.jpg',\n",
              " 'images38.jpg',\n",
              " 'images54.jpg',\n",
              " 'images66.jpg',\n",
              " 'images78.jpg',\n",
              " 'images82.jpg',\n",
              " '4737f2aa41355e5e380d9ff069f70e41.jpg',\n",
              " '481f1f87d0583457a1e973d333768f13.jpg',\n",
              " '458dceb36aec7d4cd681c3715721741c.jpg',\n",
              " '4805cea53d67566c5d284128db2741ec.jpg',\n",
              " '4a572da61ad398b73af7a747671a11a6.jpg',\n",
              " '4b65bbf0f7e5a0b390495887f819438a.jpg',\n",
              " '4a037426e7d488fda010ebfcb7e9acc7.jpg',\n",
              " '48fe3871f342fbdbbb91846a8882e0da.jpg',\n",
              " '4849f6e2720f71c6e182e323eb775f26.jpg',\n",
              " '4d8869c347f6d207068c505a2d486b93.jpg',\n",
              " '4d77251e443d51cbd8fde7b965dc00e9.jpg',\n",
              " '4ddebea6cabd98ade8c770c06d8d9f36.jpg',\n",
              " '50110dd525e35ce588cbce3c8c924fbf.jpg',\n",
              " '5.jpg',\n",
              " '50096ce4732ba7dfb79a8788cb25000e.jpg',\n",
              " '4f69655a629c942bf913c3d4ea262b77.jpg',\n",
              " '50c4e480ca5ccf5c63e4b56291d07a92.jpg',\n",
              " '51a2f6ccda52da5f622e58ce0677d9e6.jpg',\n",
              " '535e68a83076021bb0b86eaa427346e4.jpg',\n",
              " '52591c579022185b723d415fd0d92d9f.jpg',\n",
              " '539624be45174ee9875ed3df875b157c.jpg',\n",
              " '556a93c91a785f7cd0b8b5d8f1612156.jpg',\n",
              " '55e49b872f7b3cb39684087f1096a3ad.jpg',\n",
              " '560a69bff01afb06263f0f5b4d6bc6bc.jpg',\n",
              " '55c93ad299833caeccfb54e7124954db.jpg',\n",
              " '5a5a67c9aa85ab29cf6ebd0eaf945b76.jpg',\n",
              " '57b25ee7a0a2c3b6facb0e9a886cdb25.jpg',\n",
              " '5bdf849b48779d9cfea0a4a2f6c1f0dd.jpg',\n",
              " '5b48c6be446c046cd05ecd60ea557208.jpg',\n",
              " '5b9e95b104f5b3d21f0918f3258fb11a.jpg',\n",
              " '5dbcf195c90925849b984209f1d07a27.jpg',\n",
              " '5db30341a32cf5af7a9c1b3952a2889c.jpg',\n",
              " '5da45959b6ec7a41c0493bbfd352b265.jpg',\n",
              " '602560fd911df0aa17dd26849c9cf6f4.jpg',\n",
              " '643165014a55c5105a70d465a28da452.jpg',\n",
              " '626411fcf8729bed0f5d71c5d06f3d09.jpg',\n",
              " '66d0f7089926a8ea2b137b8cd55361c0.jpg',\n",
              " '662fc12512e9f03834ba688e6e8b3913.jpg',\n",
              " '67c91772596658926f2adbff4ee03957.jpg',\n",
              " '66dc9044ba880e83e54aae0fa0977f61.jpg',\n",
              " '6a3d6378f658c1f8886cffbf1add6ea1.jpg',\n",
              " '6a89e704c0d1f6163b534208883adcec.jpg',\n",
              " '6aa2507d66dc3d6ef4fd888db4f60454.jpg',\n",
              " '692733056439fcfa5e04018de7815871.jpg',\n",
              " '691ae6d7d4831914627b269da6065c41.jpg',\n",
              " '684107780a8de7127139ade96773a70a.jpg',\n",
              " '679ab871236a62e3df92efd2dc860543.jpg',\n",
              " '6c465e4e37d02087d39d3e104d93371c.jpg',\n",
              " '6c281415bb2fc96056c17275909641c1.jpg',\n",
              " '6cb9fd2928709209a46cc16aa5bbfc31.jpg',\n",
              " '6eb54cce2e3734fc8f5fe5c13b80813a.jpg',\n",
              " '7045772f76a60f1f176dff9948d469f9.jpg',\n",
              " '6f58e0a9bd4ced785d64f282714b0b28.jpg',\n",
              " '6f8e52b379e2245ea7ecec34db529e54.jpg',\n",
              " '7100d5b5e231ab072a7b1ba492b74d15.jpg',\n",
              " '721da4eb5d66adb68315188c61be2a7a.jpg',\n",
              " '20230705249L.jpg',\n",
              " '744c71d3dfae07a780ec26519dacb983.jpg',\n",
              " '751c8af13f52680ba88771380441e8fa.jpg',\n",
              " '75cad24948d065a2e8bc9166570ee6a4.jpg',\n",
              " '7606479f5a11adbd384fa69225c21581.jpg',\n",
              " '7615293cc560fd076043262ebf88a190.jpg',\n",
              " '7868f62e67f5c924e52559422509874e.jpg',\n",
              " '76f162a9427fd2d45e7ba1dbd70f4838.jpg',\n",
              " '530573303_38c814312d_o.jpg',\n",
              " '79a3a9ace76f75a633ff3354bbe3affd.jpg',\n",
              " '7721f9eecde6d445cac694478c4953f8.jpg',\n",
              " '7d62921778263c6b579cbb27ef3813b1.jpg',\n",
              " '7cf31278b27967f44472d2973c6fbd6b.jpg',\n",
              " '7d3e67b1f8c921b2d599ef7e6a17db59.jpg',\n",
              " '7ef8df753bd549011e8f6f64e1a919cc.jpg',\n",
              " '81446a3d9567c97359ce86f718f25deb.jpg',\n",
              " '7f0f09e080f197cad39ab9230855a813.jpg',\n",
              " '7f7a7f8cbb356aefa5b59ee2beaf0aae.jpg',\n",
              " 'ANLem4bTF54ggRx8hH9iuVqsx-Kc7iVFnWgSlJF7bQjUQgs64-c-mo.jpg',\n",
              " '848116776dfbbd183175e9f71124db9a.jpg',\n",
              " '83e61c3fe65999ef9250ba182aa76a1f.jpg',\n",
              " '83d94a28e6886835028ce62e92dbcb28.jpg',\n",
              " '85525db9e0c492dee26e11616d7767f9.jpg',\n",
              " 'Bumburet-Chilam-Joshi-Festival.jpg',\n",
              " 'D7oaj_fXYAUVwRS.jpg',\n",
              " '86a9b383b960a05bf7f60716760b2868.jpg',\n",
              " 'DSC_4371.jpg',\n",
              " '8897161000066f92ae27d9eb1d329631.jpg',\n",
              " '8bddac50a2b6c09cc24dd78e1008e2aa.jpg',\n",
              " '8b17a3d0aba997d7cdf638e4ead136e6.jpg',\n",
              " '8bb55be09ff9e6dec8b473ed282ea618.jpg',\n",
              " '8da597de15ee6ba7c03ce477054e899d.jpg',\n",
              " '8e042bda6bac2d7b672ba9843a6d0fda.jpg',\n",
              " '8d4a3206b2551b2e6f23195f938f0f98.jpg',\n",
              " '8fb364655038904fd453f8ef9680c88a.jpg',\n",
              " '8faad326bfaeb6c3105abc69eda9241f.jpg',\n",
              " '8fd7e90fba8674846c82e96f7f6f35f5.jpg',\n",
              " 'ances-embroidery-and-heavy-cowrie-hats-or-porcelain-decorative-coins-2BKJD16.jpg',\n",
              " '903f0ef884f57b915cc24c19242e8510.jpg',\n",
              " 'a478366f3f9a5ea09bc930d301691c6e.jpg',\n",
              " 'bamburet-kpkpakistan-05152018-kalash-woman-260nw-2354095881.jpg',\n",
              " '92ee882c59cc6de9a83d5df0ef796ced.jpg',\n",
              " 'ba085af9dc2cf37262e6e61ed308a802.png',\n",
              " '90f70ee3d5af17053f5a6a7cb053e8d7.jpg',\n",
              " '919dcc88f5bcdf6ebbc6293db7572290.jpg',\n",
              " '934f2c197530f36de046f508ba2cee23.jpg',\n",
              " '92bd95ea67c0495e9f0828e9cdee8c10.jpg',\n",
              " '94682a30bd249d54ebf26316c3d16baa.jpg',\n",
              " 'formidable-looking-kalash-woman-in-traditional-dress-on-the-steps-D38WD3.jpg',\n",
              " 'd2598315_43d313e6d9afcfa0217815a3e12163c6ef3d5b96.jpeg',\n",
              " 'from-kalash-valley-to-australia-saira-jabeen-s-challenging-journey-1700559901-9802.jpg',\n",
              " 'er-wear-ladies-t-shirt-night-wear-collection-supplier-2-2023-04-21_14_43_14.jpeg',\n",
              " 'hqdefault.jpg',\n",
              " 'il_fullxfull.1550853815_jzdr.jpg',\n",
              " '98a70756b5877c6d068a4f91c70bddde.jpg',\n",
              " 'image11.jpeg',\n",
              " '9b4e6b2f7641ea0ffac2830855eadc7c.jpg',\n",
              " '9af7b95b111799e6101d29f3485bcbbe.jpg',\n",
              " '9e112c67f73e54686ebebf68984f7ba0.jpg',\n",
              " '9ad52322609a9df662ba66d013ceb559.jpg',\n",
              " 'a1338fd403e51853645d43dfd61efd16.jpg',\n",
              " 'image20.jpeg',\n",
              " 'a1c32dbc2d9935dcd491c4c1c1e02316.jpg',\n",
              " 'image21.jpeg',\n",
              " '9f88435f633a142335e7ac905531d2a7.jpg',\n",
              " 'a11f0cc6448c62f92c8360f3e4c9be57.jpg',\n",
              " '9fc260cdd817bfd1b823e1316b23775f.jpg',\n",
              " 'a2806019cd2f11eeaf08dd82b345f6f6.jpg',\n",
              " 'a43948ac6af441cf038ac42c6557fe9c.jpg',\n",
              " 'a73f722979200481e0a544f7aa45b4a5.jpg',\n",
              " 'a5a39b7a8321e1d4ed68999e54e3bd9c.jpg',\n",
              " 'a72f1f9702ca9d47c2b5e499eca47a80.jpg',\n",
              " 'a783c5f2d25cd2466808464bd57aa19c.jpg',\n",
              " 'a7e3da77cc6b1e413e00e44fdb97e353.jpg',\n",
              " 'images107.jpg',\n",
              " 'a77856d1ec67f42330dc404d00e01a64.jpg',\n",
              " 'a745e5a62827f131bb2c3282b379d01f.jpg',\n",
              " 'a74ea14b6d76a8dc16a56d3b9f40beb8.jpg',\n",
              " 'images117.jpg',\n",
              " 'images116.jpg',\n",
              " 'images13.jpg',\n",
              " 'images124.jpg',\n",
              " 'images125.jpg',\n",
              " 'images3.jpg',\n",
              " 'images27.jpg',\n",
              " 'images29.jpg',\n",
              " 'images28.jpg',\n",
              " 'images4.jpg',\n",
              " 'images43.jpg',\n",
              " 'images53.jpg',\n",
              " 'images52.jpg',\n",
              " 'images67.jpg',\n",
              " 'images68.jpg',\n",
              " 'images77.jpg',\n",
              " 'images91.jpg',\n",
              " 'images92.jpg',\n",
              " 'images93.jpg',\n",
              " 'BKS-8225-_F.jpg',\n",
              " 'BK-S-1033_2.jpg',\n",
              " 'Boys-kurta-6-scaled.jpg',\n",
              " 'Boy-Wash-n-Wear-Kameez-Shalwar-White-2_1024x1024.jpg',\n",
              " 'C-1175-13999-7.jpg',\n",
              " 'Brown-Kameez-and-Shalwar.jpg',\n",
              " 'Boys_Eid_Shalwar_Kameez_in_Black_Color_1_1200x630.jpg',\n",
              " 'Cricketer-Wahab-Riaz-in-Party-Shalwar-Kameez.jpg',\n",
              " 'Cappuccino-Kameez-Shalwar-1-300x450.jpg',\n",
              " 'DRM5194.jpg',\n",
              " 'D022202-l-brown02_800x.jpg',\n",
              " 'Cocktail-Blue-Ghair-Kameez-Shalwar-For-Kids-Balochi-5.jpg',\n",
              " 'DRM5725.jpg',\n",
              " 'DSC01452.jpg',\n",
              " 'DSC03347-1.jpg',\n",
              " 'DSC01770.jpg',\n",
              " 'DRM5718.jpg',\n",
              " 'DSC_3087_740x.jpg',\n",
              " 'DSC_2850_740x.jpg',\n",
              " 'EG3202-D-BLUE-RS-6690-02.jpg',\n",
              " 'EG3205-D-GREY-RS-6490-02.jpg',\n",
              " 'EG3204-C-GREY-03.jpg',\n",
              " 'EidShalwarKameezforMen_1_61f2773d-1ef7-4c6a-98eb-5b333b09843c_1024x1024.jpg',\n",
              " 'Gents_Formal_Designer_Shalwar_Kameez_Sea_green_Gala_Work_620x.jpeg',\n",
              " 'GTS674_4.jpg',\n",
              " 'e01c3a8f6a83bcbf4f156fb0f814f04e.jpg',\n",
              " 'egypt_ceramic_blue_jjks-8208-d_3_.jpg',\n",
              " 'ec1cdc5081cdf180c3bfd8d0ba89abc8.jpg',\n",
              " 'cairo_navy_blue_1_.jpg',\n",
              " 'gry-1_1024x1024.jpg',\n",
              " 'grayshalwarkameezformen_1445x.jpg']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "data_dir = '/content/drive/MyDrive/milito'\n",
        "os.listdir(data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_89gXl_MzOJ",
        "outputId": "5664bf29-0d5c-4636-9b8e-388214760d65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: ['12.jpeg', '84.jpeg', '29.jpeg', '112.jpeg', '129.jpeg', '201.jpeg', '301.jpeg', '433.jpeg', '1774.jpeg', '1759.jpeg', '1746.jpeg', '1790.jpeg', '1813.jpeg', '2564.jpeg', '2636.jpeg', '2847.jpeg', '0.jpeg', '3723.jpeg', '4347.jpeg', '4323.jpeg', '4330.jpeg', '4315.jpeg', '4588.jpeg', '4626.jpeg', '4590.jpeg', '4625.jpeg', '7045.jpeg', '7037.jpeg', '7054.jpeg', '7061.jpeg', '40.jpeg', '144.jpeg', '156.jpeg', '173.jpeg', '229.jpeg', '316.jpeg', '289.jpeg', '478.jpeg', '477.jpeg', '490.jpeg', '1758.jpeg', '1775.jpeg', '1883.jpeg', '1855.jpeg', '1867.jpeg', '1800.jpeg', '2658.jpeg', '2565.jpeg', '2596.jpeg', '2694.jpeg', '3165.jpeg', '3164.jpeg', '3200.jpeg', '3734.jpeg', '4355.jpeg', '4540.jpeg', '4589.jpeg', '7038.jpeg', '7053.jpeg', '13.jpeg', '28.jpeg', '85.jpeg', '261.jpeg', '328.jpeg', '345.jpeg', '344.jpeg', '432.jpeg', '462.jpeg', '1773.jpeg', '1818.jpeg', '1899.jpeg', '2539.jpeg', '3238.jpeg', '3691.jpeg', '3707.jpeg', '3766.jpeg', '4354.jpeg', '4534.jpeg', '4604.jpeg', '57.jpeg', '117.jpeg', '244.jpeg', '256.jpeg', '300.jpeg', '333.jpeg', '418.jpeg', '444.jpeg', '1763.jpeg', '1812.jpeg', '1785.jpeg', '1827.jpeg', '2600.jpeg', '2601.jpeg', '2668.jpeg', '2695.jpeg', '3272.jpeg', '3755.jpeg', '4573.jpeg', '4602.jpeg', '7046.jpeg', '1.jpeg', '200.jpeg', '417.jpeg', '461.jpeg', '489.jpeg', '1844.jpeg', '1802.jpeg', '2659.jpeg', '3237.jpeg', '3739.jpeg', '4338.jpeg', '4564.jpeg', '4574.jpeg', '4614.jpeg', '7062.jpeg', '100.jpeg', '128.jpeg', '288.jpeg', '216.jpeg', '217.jpeg', '228.jpeg', '273.jpeg', '317.jpeg', '1840.jpeg', '1801.jpeg', '1882.jpeg', '2529.jpeg', '2574.jpeg', '2623.jpeg', '2693.jpeg', '2845.jpeg', '3193.jpeg', '3686.jpeg', '3702.jpeg', '3675.jpeg', '4314.jpeg', '4322.jpeg', '45.jpeg', '72.jpeg', '73.jpeg', '56.jpeg', '157.jpeg', '145.jpeg', '172.jpeg', '101.jpeg', '245.jpeg', '189.jpeg', '272.jpeg', '460.jpeg', '472.jpeg', '1747.jpeg', '1830.jpeg', '1884.jpeg', '1856.jpeg', '1872.jpeg', '1829.jpeg', '2632.jpeg', '3771.jpeg', '3750.jpeg', '4346.jpeg', '4339.jpeg', '4533.jpeg', '4603.jpeg', '4618.jpeg', '4613.jpeg', '7029.jpeg', '7030.jpeg', '7070.jpeg', '184.jpeg', '416.jpeg', '450.jpeg', '488.jpeg', '445.jpeg', '434.jpeg', '1786.jpeg', '1871.jpeg', '1845.jpeg', '1857.jpeg', '1748.jpeg', '1894.jpeg', '1898.jpeg', '2538.jpeg', '2846.jpeg', '3718.jpg', '4331.jpeg', '4624.jpeg', '7069.jpeg', '14075060472_974ee0b00b_b.jpg', '403849179-800x600.jpeg', '403849178-800x600.jpeg', '8572987636_9a2c56c0be.jpg', 'AJ01-1.jpg', 'Ae8d73fe0a28c4d5da548f4944e5c015eS.jpg_300x300.jpg', 'Ajrak-Frock-CU2--e25dd3b-behbud-boutique.jpg', 'Ajrak-lawn-dress.jpg', 'Ajrak-kurti-For-Baby-Girl-2-scaled.jpg', 'Ajrak20Shawl20Blue-900x900.jpg', 'Ajrak_Sindhi_3_grande.jpg', 'BlackAjrakFrockFront.jpg', 'BAJK__w1_all_set223_girldress_front__2020-11-13-19-6-50__1365X2048.jpg', 'BNBK-13.jpeg', 'FB-IMG_1695759841399_554x.jpg', 'FB_IMG_1569339504704.jpg', 'az-large-3165718.jpg', 'ajrak_fe0680fc-7b89-42e8-a89a-78e9db7417cc.jpg', 'cultural-photos-290.jpg', 'e7584fc0f8336678e17441dc4c227806.jpg', 'hq2.jpg', 'hq720.jpg', 'fd462773cc76d63f01d5515fd060f2ae.jpg', 'il_570xN.5381670830_o5st.jpg', 'image10.jpeg', 'il_fullxfull.3205221479_c80i.jpg', 'image25.jpeg', 'images102.jpg', 'image9.jpeg', 'images106.jpg', 'images118.jpg', 'images126.jpg', 'images132.jpg', 'images134.jpg', 'images133.jpg', 'images140.jpg', 'images141.jpg', 'images142.jpg', 'images14.jpg', 'images18.jpg', 'images2.jpg', 'images42.jpg', 'images38.jpg', 'images54.jpg', 'images66.jpg', 'images78.jpg', 'images82.jpg', '4737f2aa41355e5e380d9ff069f70e41.jpg', '481f1f87d0583457a1e973d333768f13.jpg', '458dceb36aec7d4cd681c3715721741c.jpg', '4805cea53d67566c5d284128db2741ec.jpg', '4a572da61ad398b73af7a747671a11a6.jpg', '4b65bbf0f7e5a0b390495887f819438a.jpg', '4a037426e7d488fda010ebfcb7e9acc7.jpg', '48fe3871f342fbdbbb91846a8882e0da.jpg', '4849f6e2720f71c6e182e323eb775f26.jpg', '4d8869c347f6d207068c505a2d486b93.jpg', '4d77251e443d51cbd8fde7b965dc00e9.jpg', '4ddebea6cabd98ade8c770c06d8d9f36.jpg', '50110dd525e35ce588cbce3c8c924fbf.jpg', '5.jpg', '50096ce4732ba7dfb79a8788cb25000e.jpg', '4f69655a629c942bf913c3d4ea262b77.jpg', '50c4e480ca5ccf5c63e4b56291d07a92.jpg', '51a2f6ccda52da5f622e58ce0677d9e6.jpg', '535e68a83076021bb0b86eaa427346e4.jpg', '52591c579022185b723d415fd0d92d9f.jpg', '539624be45174ee9875ed3df875b157c.jpg', '556a93c91a785f7cd0b8b5d8f1612156.jpg', '55e49b872f7b3cb39684087f1096a3ad.jpg', '560a69bff01afb06263f0f5b4d6bc6bc.jpg', '55c93ad299833caeccfb54e7124954db.jpg', '5a5a67c9aa85ab29cf6ebd0eaf945b76.jpg', '57b25ee7a0a2c3b6facb0e9a886cdb25.jpg', '5bdf849b48779d9cfea0a4a2f6c1f0dd.jpg', '5b48c6be446c046cd05ecd60ea557208.jpg', '5b9e95b104f5b3d21f0918f3258fb11a.jpg', '5dbcf195c90925849b984209f1d07a27.jpg', '5db30341a32cf5af7a9c1b3952a2889c.jpg', '5da45959b6ec7a41c0493bbfd352b265.jpg', '602560fd911df0aa17dd26849c9cf6f4.jpg', '643165014a55c5105a70d465a28da452.jpg', '626411fcf8729bed0f5d71c5d06f3d09.jpg', '66d0f7089926a8ea2b137b8cd55361c0.jpg', '662fc12512e9f03834ba688e6e8b3913.jpg', '67c91772596658926f2adbff4ee03957.jpg', '66dc9044ba880e83e54aae0fa0977f61.jpg', '6a3d6378f658c1f8886cffbf1add6ea1.jpg', '6a89e704c0d1f6163b534208883adcec.jpg', '6aa2507d66dc3d6ef4fd888db4f60454.jpg', '692733056439fcfa5e04018de7815871.jpg', '691ae6d7d4831914627b269da6065c41.jpg', '684107780a8de7127139ade96773a70a.jpg', '679ab871236a62e3df92efd2dc860543.jpg', '6c465e4e37d02087d39d3e104d93371c.jpg', '6c281415bb2fc96056c17275909641c1.jpg', '6cb9fd2928709209a46cc16aa5bbfc31.jpg', '6eb54cce2e3734fc8f5fe5c13b80813a.jpg', '7045772f76a60f1f176dff9948d469f9.jpg', '6f58e0a9bd4ced785d64f282714b0b28.jpg', '6f8e52b379e2245ea7ecec34db529e54.jpg', '7100d5b5e231ab072a7b1ba492b74d15.jpg', '721da4eb5d66adb68315188c61be2a7a.jpg', '20230705249L.jpg', '744c71d3dfae07a780ec26519dacb983.jpg', '751c8af13f52680ba88771380441e8fa.jpg', '75cad24948d065a2e8bc9166570ee6a4.jpg', '7606479f5a11adbd384fa69225c21581.jpg', '7615293cc560fd076043262ebf88a190.jpg', '7868f62e67f5c924e52559422509874e.jpg', '76f162a9427fd2d45e7ba1dbd70f4838.jpg', '530573303_38c814312d_o.jpg', '79a3a9ace76f75a633ff3354bbe3affd.jpg', '7721f9eecde6d445cac694478c4953f8.jpg', '7d62921778263c6b579cbb27ef3813b1.jpg', '7cf31278b27967f44472d2973c6fbd6b.jpg', '7d3e67b1f8c921b2d599ef7e6a17db59.jpg', '7ef8df753bd549011e8f6f64e1a919cc.jpg', '81446a3d9567c97359ce86f718f25deb.jpg', '7f0f09e080f197cad39ab9230855a813.jpg', '7f7a7f8cbb356aefa5b59ee2beaf0aae.jpg', 'ANLem4bTF54ggRx8hH9iuVqsx-Kc7iVFnWgSlJF7bQjUQgs64-c-mo.jpg', '848116776dfbbd183175e9f71124db9a.jpg', '83e61c3fe65999ef9250ba182aa76a1f.jpg', '83d94a28e6886835028ce62e92dbcb28.jpg', '85525db9e0c492dee26e11616d7767f9.jpg', 'Bumburet-Chilam-Joshi-Festival.jpg', 'D7oaj_fXYAUVwRS.jpg', '86a9b383b960a05bf7f60716760b2868.jpg', 'DSC_4371.jpg', '8897161000066f92ae27d9eb1d329631.jpg', '8bddac50a2b6c09cc24dd78e1008e2aa.jpg', '8b17a3d0aba997d7cdf638e4ead136e6.jpg', '8bb55be09ff9e6dec8b473ed282ea618.jpg', '8da597de15ee6ba7c03ce477054e899d.jpg', '8e042bda6bac2d7b672ba9843a6d0fda.jpg', '8d4a3206b2551b2e6f23195f938f0f98.jpg', '8fb364655038904fd453f8ef9680c88a.jpg', '8faad326bfaeb6c3105abc69eda9241f.jpg', '8fd7e90fba8674846c82e96f7f6f35f5.jpg', 'ances-embroidery-and-heavy-cowrie-hats-or-porcelain-decorative-coins-2BKJD16.jpg', '903f0ef884f57b915cc24c19242e8510.jpg', 'a478366f3f9a5ea09bc930d301691c6e.jpg', 'bamburet-kpkpakistan-05152018-kalash-woman-260nw-2354095881.jpg', '92ee882c59cc6de9a83d5df0ef796ced.jpg', 'ba085af9dc2cf37262e6e61ed308a802.png', '90f70ee3d5af17053f5a6a7cb053e8d7.jpg', '919dcc88f5bcdf6ebbc6293db7572290.jpg', '934f2c197530f36de046f508ba2cee23.jpg', '92bd95ea67c0495e9f0828e9cdee8c10.jpg', '94682a30bd249d54ebf26316c3d16baa.jpg', 'formidable-looking-kalash-woman-in-traditional-dress-on-the-steps-D38WD3.jpg', 'd2598315_43d313e6d9afcfa0217815a3e12163c6ef3d5b96.jpeg', 'from-kalash-valley-to-australia-saira-jabeen-s-challenging-journey-1700559901-9802.jpg', 'er-wear-ladies-t-shirt-night-wear-collection-supplier-2-2023-04-21_14_43_14.jpeg', 'hqdefault.jpg', 'il_fullxfull.1550853815_jzdr.jpg', '98a70756b5877c6d068a4f91c70bddde.jpg', 'image11.jpeg', '9b4e6b2f7641ea0ffac2830855eadc7c.jpg', '9af7b95b111799e6101d29f3485bcbbe.jpg', '9e112c67f73e54686ebebf68984f7ba0.jpg', '9ad52322609a9df662ba66d013ceb559.jpg', 'a1338fd403e51853645d43dfd61efd16.jpg', 'image20.jpeg', 'a1c32dbc2d9935dcd491c4c1c1e02316.jpg', 'image21.jpeg', '9f88435f633a142335e7ac905531d2a7.jpg', 'a11f0cc6448c62f92c8360f3e4c9be57.jpg', '9fc260cdd817bfd1b823e1316b23775f.jpg', 'a2806019cd2f11eeaf08dd82b345f6f6.jpg', 'a43948ac6af441cf038ac42c6557fe9c.jpg', 'a73f722979200481e0a544f7aa45b4a5.jpg', 'a5a39b7a8321e1d4ed68999e54e3bd9c.jpg', 'a72f1f9702ca9d47c2b5e499eca47a80.jpg', 'a783c5f2d25cd2466808464bd57aa19c.jpg', 'a7e3da77cc6b1e413e00e44fdb97e353.jpg', 'images107.jpg', 'a77856d1ec67f42330dc404d00e01a64.jpg', 'a745e5a62827f131bb2c3282b379d01f.jpg', 'a74ea14b6d76a8dc16a56d3b9f40beb8.jpg', 'images117.jpg', 'images116.jpg', 'images13.jpg', 'images124.jpg', 'images125.jpg', 'images3.jpg', 'images27.jpg', 'images29.jpg', 'images28.jpg', 'images4.jpg', 'images43.jpg', 'images53.jpg', 'images52.jpg', 'images67.jpg', 'images68.jpg', 'images77.jpg', 'images91.jpg', 'images92.jpg', 'images93.jpg', 'BKS-8225-_F.jpg', 'BK-S-1033_2.jpg', 'Boys-kurta-6-scaled.jpg', 'Boy-Wash-n-Wear-Kameez-Shalwar-White-2_1024x1024.jpg', 'C-1175-13999-7.jpg', 'Brown-Kameez-and-Shalwar.jpg', 'Boys_Eid_Shalwar_Kameez_in_Black_Color_1_1200x630.jpg', 'Cricketer-Wahab-Riaz-in-Party-Shalwar-Kameez.jpg', 'Cappuccino-Kameez-Shalwar-1-300x450.jpg', 'DRM5194.jpg', 'D022202-l-brown02_800x.jpg', 'Cocktail-Blue-Ghair-Kameez-Shalwar-For-Kids-Balochi-5.jpg', 'DRM5725.jpg', 'DSC01452.jpg', 'DSC03347-1.jpg', 'DSC01770.jpg', 'DRM5718.jpg', 'DSC_3087_740x.jpg', 'DSC_2850_740x.jpg', 'EG3202-D-BLUE-RS-6690-02.jpg', 'EG3205-D-GREY-RS-6490-02.jpg', 'EG3204-C-GREY-03.jpg', 'EidShalwarKameezforMen_1_61f2773d-1ef7-4c6a-98eb-5b333b09843c_1024x1024.jpg', 'Gents_Formal_Designer_Shalwar_Kameez_Sea_green_Gala_Work_620x.jpeg', 'GTS674_4.jpg', 'e01c3a8f6a83bcbf4f156fb0f814f04e.jpg', 'egypt_ceramic_blue_jjks-8208-d_3_.jpg', 'ec1cdc5081cdf180c3bfd8d0ba89abc8.jpg', 'cairo_navy_blue_1_.jpg', 'gry-1_1024x1024.jpg', 'grayshalwarkameezformen_1445x.jpg']\n",
            "Number of classes: 418\n"
          ]
        }
      ],
      "source": [
        "dataset_path = \"/content/drive/MyDrive/milito\"\n",
        "classes = os.listdir(dataset_path)\n",
        "print(\"Classes:\", classes)\n",
        "print(\"Number of classes:\", len(classes))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b5jbTDGMc_R"
      },
      "source": [
        "Resizing data / preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "oIG3R7loMWri",
        "outputId": "7e9d00a9-6ca3-4ef6-9902-3e8ac2bd34b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 0 files belonging to 0 classes.\n",
            "Using 0 files for training.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "No images found in directory /content/drive/MyDrive/milito. Allowed formats: ('.bmp', '.gif', '.jpeg', '.jpg', '.png')",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-6-3805632957.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_dataset_from_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'training'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_dataset_from_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'validation'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/image_dataset_utils.py\u001b[0m in \u001b[0;36mimage_dataset_from_directory\u001b[0;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, pad_to_aspect_ratio, data_format, verbose)\u001b[0m\n\u001b[1;32m    327\u001b[0m         )\n\u001b[1;32m    328\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mimage_paths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    330\u001b[0m                 \u001b[0;34mf\"No images found in directory {directory}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m                 \u001b[0;34mf\"Allowed formats: {ALLOWLIST_FORMATS}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: No images found in directory /content/drive/MyDrive/milito. Allowed formats: ('.bmp', '.gif', '.jpeg', '.jpg', '.png')"
          ]
        }
      ],
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "train_data = keras.utils.image_dataset_from_directory(data_dir, validation_split = 0.1, subset = 'training', seed = 1, shuffle = True, batch_size = 16, image_size=(256,256))\n",
        "\n",
        "test_data = keras.utils.image_dataset_from_directory(data_dir, validation_split = 0.1, subset = 'validation', seed = 1, shuffle = True, batch_size = 16, image_size=(256,256))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95srxy57Mj2B"
      },
      "outputs": [],
      "source": [
        "filenames = pathlib.Path(data_dir)\n",
        "for label in train_data.class_names :\n",
        "    images = list(filenames.glob(f'{label}/*'))\n",
        "    print(f'{label} : {len(images)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HT24DraJNTqT"
      },
      "outputs": [],
      "source": [
        "train_set = train_data.take(150)\n",
        "val_set = train_data.skip(150)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BsP0tR8BMocK"
      },
      "outputs": [],
      "source": [
        "train_data.cardinality().numpy(),  test_data.cardinality().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TF0pMfaWNRGG"
      },
      "outputs": [],
      "source": [
        "train_set.cardinality().numpy(), val_set.cardinality().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07HwaoNINEiq"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Get class names from `image_dataset_from_directory`\n",
        "class_names = train_data.class_names  # Correct way to extract class names\n",
        "\n",
        "plt.figure(figsize=(15, 15))\n",
        "\n",
        "for images, labels in train_set.take(1):  # Take one batch\n",
        "    for i in range(15):\n",
        "        index = random.randint(0, len(images) - 1)  # Ensure correct indexing\n",
        "        ax = plt.subplot(3, 5, i + 1)\n",
        "\n",
        "        plt.imshow(images[index].numpy().astype(\"uint8\"))  # Convert tensor to NumPy\n",
        "        plt.title(class_names[labels[index].numpy()])  # Correct class name extraction\n",
        "        plt.axis(\"off\")  # Hide axis\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ue6Qh1UePFR"
      },
      "source": [
        "ekhan theke shuru"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1BKIdcLNMIq"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "J2c3jlP_NlAG"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers, models\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHUKJwtEQ8bg"
      },
      "source": [
        "Faiss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WS4GN_1kQ6i5"
      },
      "outputs": [],
      "source": [
        "!pip install faiss-cpu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24zgXMoMcUew"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibJ0K9T7cZMk"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define dataset paths\n",
        "IMAGES_PATH = '/content/drive/MyDrive/test_Sharee'  # Update with correct dataset path\n",
        "OUTPUT_INDEX_PATH = '/content/drive/MyDrive/vector.index'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEEp-L1scc57"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load CLIP model\n",
        "model = SentenceTransformer('clip-ViT-B-32')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7caaJl-ceMS"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_label_from_path(img_path):\n",
        "    \"\"\"Extracts class label from folder name.\"\"\"\n",
        "    return os.path.basename(os.path.dirname(img_path))\n",
        "\n",
        "def generate_clip_embeddings(images_path, model):\n",
        "    \"\"\"Generates embeddings and labels.\"\"\"\n",
        "    image_paths = glob(os.path.join(images_path, '**/*.jpg'), recursive=True)\n",
        "\n",
        "    embeddings = []\n",
        "    labels = []\n",
        "\n",
        "    for img_path in image_paths:\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "            embedding = model.encode(image)\n",
        "            embeddings.append(embedding)\n",
        "            labels.append(get_label_from_path(img_path))  # Extract class label\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {img_path}: {e}\")\n",
        "\n",
        "    return embeddings, image_paths, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jn0qqdPzcn1g"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Generate embeddings and labels\n",
        "embeddings, image_paths, labels = generate_clip_embeddings(IMAGES_PATH, model)\n",
        "\n",
        "# Convert labels to numeric classes\n",
        "unique_classes = list(set(labels))\n",
        "class_to_index = {cls: i for i, cls in enumerate(unique_classes)}\n",
        "numeric_labels = np.array([class_to_index[label] for label in labels])\n",
        "\n",
        "# Check class distribution\n",
        "print(f\"Class Distribution: {Counter(labels)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAECT6k_cwSp"
      },
      "outputs": [],
      "source": [
        "# Split dataset into train and test (ensuring balanced classes)\n",
        "train_emb, test_emb, train_paths, test_paths, train_labels, test_labels = train_test_split(\n",
        "    embeddings, image_paths, numeric_labels, test_size=0.2, stratify=numeric_labels, random_state=42\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Wi62NG2c_Su"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def create_faiss_index(embeddings, image_paths, output_path):\n",
        "    \"\"\"Creates FAISS index using ONLY training embeddings.\"\"\"\n",
        "    if len(embeddings) == 0:\n",
        "        raise ValueError(\"No embeddings found. Check dataset path.\")\n",
        "\n",
        "    dimension = len(embeddings[0])\n",
        "    index = faiss.IndexFlatIP(dimension)  # Inner product for similarity\n",
        "    index = faiss.IndexIDMap(index)\n",
        "\n",
        "    vectors = np.array(embeddings).astype(np.float32)\n",
        "    ids = np.arange(len(embeddings)).astype(np.int64)  # Unique IDs\n",
        "\n",
        "    index.add_with_ids(vectors, ids)\n",
        "    faiss.write_index(index, output_path)\n",
        "\n",
        "    with open(output_path + '.paths', 'w') as f:\n",
        "        for img_path in image_paths:\n",
        "            f.write(img_path + '\\n')\n",
        "\n",
        "    print(f\"FAISS index saved at: {output_path}\")\n",
        "    return index\n",
        "\n",
        "# Ensure FAISS index is built ONLY on training embeddings\n",
        "index = create_faiss_index(train_emb, train_paths, OUTPUT_INDEX_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLfQ5VtrdtrM"
      },
      "outputs": [],
      "source": [
        "\n",
        "def load_faiss_index(index_path):\n",
        "    \"\"\"Loads FAISS index and image paths.\"\"\"\n",
        "    index = faiss.read_index(index_path)\n",
        "    with open(index_path + '.paths', 'r') as f:\n",
        "        image_paths = [line.strip() for line in f]\n",
        "    print(f\"FAISS index loaded from: {index_path}\")\n",
        "    return index, image_paths\n",
        "\n",
        "index, image_paths = load_faiss_index(OUTPUT_INDEX_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpOGpa6Gd5Z-"
      },
      "outputs": [],
      "source": [
        "\n",
        "def retrieve_similar_images(query, model, index, image_paths, top_k=3):\n",
        "    \"\"\"Retrieves similar images using FAISS.\"\"\"\n",
        "    if not os.path.exists(query):\n",
        "        print(f\"Error: Query image '{query}' not found. Please provide a valid image path.\")\n",
        "        return None, None\n",
        "\n",
        "    query_image = Image.open(query).convert('RGB')\n",
        "    query_features = model.encode(query_image).astype(np.float32).reshape(1, -1)\n",
        "    distances, indices = index.search(query_features, top_k)\n",
        "\n",
        "    retrieved_images = [image_paths[int(idx)] for idx in indices[0]]\n",
        "    return query_image, retrieved_images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vxz-3LrMeLD9"
      },
      "outputs": [],
      "source": [
        "\n",
        "def visualize_results(query, retrieved_images):\n",
        "    \"\"\"Displays query image and retrieved images.\"\"\"\n",
        "    if query is None or retrieved_images is None:\n",
        "        print(\"Error: No images retrieved.\")\n",
        "        return\n",
        "\n",
        "    fig, ax = plt.subplots(1, len(retrieved_images) + 1, figsize=(15, 5))\n",
        "\n",
        "    ax[0].imshow(query)\n",
        "    ax[0].set_title('Query')\n",
        "    ax[0].axis('off')\n",
        "\n",
        "    for i, img_path in enumerate(retrieved_images):\n",
        "        image = Image.open(img_path)\n",
        "        ax[i + 1].imshow(image)\n",
        "        ax[i + 1].set_title(f\"Match {i + 1}\")\n",
        "        ax[i + 1].axis('off')\n",
        "\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gF5euycg3wuy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mslmz0mlQj7p"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Search with a reference image (Ensure path is correct)\n",
        "query_path = '/content/drive/MyDrive/c9df058e-25c0-4ba6-81da-6494ed33a42a.jpg'  # Update this\n",
        "query, retrieved_images = retrieve_similar_images(query_path, model, index, image_paths, top_k=4)\n",
        "\n",
        "if query is not None:\n",
        "    visualize_results(query, retrieved_images)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DZLRo5xQj4O"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLVklrhEQj1n"
      },
      "outputs": [],
      "source": [
        "def evaluate_faiss(index, query_vectors, ground_truth, k=5):\n",
        "    num_queries = len(query_vectors)\n",
        "\n",
        "    if num_queries == 0:\n",
        "        print(\"No queries to evaluate.\")\n",
        "        return 0, 0, 0\n",
        "\n",
        "    if index.ntotal == 0:\n",
        "        print(\"FAISS index is empty. Add vectors before evaluating.\")\n",
        "        return 0, 0, 0\n",
        "\n",
        "    print(f\"Evaluating {num_queries} queries against {index.ntotal} vectors...\")\n",
        "\n",
        "    # FAISS search\n",
        "    D, I = index.search(query_vectors, k)  # I: Retrieved indices, D: Distances\n",
        "    print(\"Sample retrieved indices:\", I[:5])  # Debug print\n",
        "\n",
        "    recall_at_k = 0\n",
        "    mean_ap = 0\n",
        "    mrr = 0\n",
        "\n",
        "    for i in range(num_queries):\n",
        "        retrieved = I[i]\n",
        "        relevant = ground_truth[i]\n",
        "\n",
        "        # Recall@K\n",
        "        if relevant in retrieved:\n",
        "            recall_at_k += 1\n",
        "\n",
        "        # Mean Average Precision\n",
        "        ap = 0\n",
        "        correct = 0\n",
        "        for rank, retrieved_idx in enumerate(retrieved, start=1):\n",
        "            if retrieved_idx == relevant:\n",
        "                correct += 1\n",
        "                ap += correct / rank\n",
        "        if correct > 0:\n",
        "            mean_ap += ap / correct\n",
        "\n",
        "        # Mean Reciprocal Rank\n",
        "        for rank, retrieved_idx in enumerate(retrieved, start=1):\n",
        "            if retrieved_idx == relevant:\n",
        "                mrr += 1 / rank\n",
        "                break\n",
        "\n",
        "    # Normalize scores\n",
        "    recall_at_k /= num_queries\n",
        "    mean_ap /= num_queries\n",
        "    mrr /= num_queries\n",
        "\n",
        "    print(f\"Recall@{k}: {recall_at_k:.4f}\")\n",
        "    print(f\"Mean Average Precision (mAP): {mean_ap:.4f}\")\n",
        "    print(f\"Mean Reciprocal Rank (MRR): {mrr:.4f}\")\n",
        "\n",
        "    return recall_at_k, mean_ap, mrr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFTdUHJUQjzY"
      },
      "outputs": [],
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "def create_faiss_index(database_vectors, metric=\"L2\"):\n",
        "    \"\"\"\n",
        "    Creates and populates a FAISS index.\n",
        "    :param database_vectors: (NumPy array) The database of vectors to index.\n",
        "    :param metric: Distance metric (\"L2\" for Euclidean, \"IP\" for Cosine similarity).\n",
        "    :return: FAISS index\n",
        "    \"\"\"\n",
        "    if database_vectors is None or len(database_vectors) == 0:\n",
        "        raise ValueError(\"Database vectors are empty or None.\")\n",
        "\n",
        "    d = database_vectors.shape[1]  # Dimensionality\n",
        "\n",
        "    if metric == \"L2\":\n",
        "        index = faiss.IndexFlatL2(d)  # L2 distance\n",
        "    elif metric == \"IP\":\n",
        "        faiss.normalize_L2(database_vectors)  # Normalize for cosine similarity\n",
        "        index = faiss.IndexFlatIP(d)  # Inner Product\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported metric. Use 'L2' or 'IP'.\")\n",
        "\n",
        "    index.add(database_vectors)  # Add vectors to index\n",
        "    return index\n",
        "\n",
        "def generate_ground_truth(index, query_vectors):\n",
        "    \"\"\"\n",
        "    Uses FAISS to find the closest ground truth for each query vector.\n",
        "    :param index: FAISS index\n",
        "    :param query_vectors: Query vectors to test.\n",
        "    :return: Ground truth indices (list)\n",
        "    \"\"\"\n",
        "    if query_vectors is None or len(query_vectors) == 0:\n",
        "        raise ValueError(\"Query vectors are empty or None.\")\n",
        "\n",
        "    _, ground_truth = index.search(query_vectors, 1)  # Get closest match per query\n",
        "    return ground_truth.flatten().tolist()  # Convert to list\n",
        "\n",
        "def evaluate_faiss(index, query_vectors, ground_truth, k=5, metric=\"L2\"):\n",
        "    \"\"\"\n",
        "    Evaluates the FAISS index using Recall@K, Mean Average Precision (mAP), and Mean Reciprocal Rank (MRR).\n",
        "    :param index: FAISS index\n",
        "    :param query_vectors: Query vectors to test.\n",
        "    :param ground_truth: Ground truth nearest neighbor indices.\n",
        "    :param k: Number of nearest neighbors to retrieve.\n",
        "    :param metric: Distance metric (\"L2\" or \"IP\").\n",
        "    :return: Recall@K, mAP, MRR\n",
        "    \"\"\"\n",
        "    num_queries = len(query_vectors)\n",
        "\n",
        "    if num_queries == 0 or index.ntotal == 0:\n",
        "        print(\"No queries or FAISS index is empty.\")\n",
        "        return 0, 0, 0\n",
        "\n",
        "    # Normalize query vectors if using cosine similarity\n",
        "    if metric == \"IP\":\n",
        "        faiss.normalize_L2(query_vectors)\n",
        "\n",
        "    print(f\"\\nEvaluating {num_queries} queries against {index.ntotal} indexed vectors...\")\n",
        "\n",
        "    # FAISS search\n",
        "    D, I = index.search(query_vectors, k)  # I: Retrieved indices, D: Distances\n",
        "\n",
        "    print(\"\\nSample Retrieved Indices:\\n\", I[:5])\n",
        "    print(\"Sample Distances:\\n\", D[:5])\n",
        "    print(\"Ground Truth Indices:\\n\", ground_truth[:5])\n",
        "\n",
        "    recall_at_k = np.mean([ground_truth[i] in I[i] for i in range(num_queries)])\n",
        "    mean_ap = np.mean([\n",
        "        np.mean([1 / (rank + 1) for rank, retrieved_idx in enumerate(I[i]) if retrieved_idx == ground_truth[i]])\n",
        "        if ground_truth[i] in I[i] else 0 for i in range(num_queries)\n",
        "    ])\n",
        "    mrr = np.mean([\n",
        "        1 / (np.where(I[i] == ground_truth[i])[0][0] + 1) if ground_truth[i] in I[i] else 0\n",
        "        for i in range(num_queries)\n",
        "    ])\n",
        "\n",
        "    print(\"\\nEvaluation Metrics:\")\n",
        "    print(f\"Recall@{k}: {recall_at_k:.4f}\")\n",
        "    print(f\"Mean Average Precision (mAP): {mean_ap:.4f}\")\n",
        "    print(f\"Mean Reciprocal Rank (MRR): {mrr:.4f}\")\n",
        "\n",
        "    return recall_at_k, mean_ap, mrr\n",
        "\n",
        "# ======  Example Usage ======\n",
        "\n",
        "# Uncomment below for reproducibility\n",
        "# np.random.seed(42)\n",
        "\n",
        "dim = 128  # Feature vector size\n",
        "num_database = 1000\n",
        "num_queries = 20  # Increased for better evaluation\n",
        "\n",
        "# Generate more realistic vectors (normal distribution)\n",
        "database_vectors = np.random.randn(num_database, dim).astype(np.float32)\n",
        "query_vectors = np.random.randn(num_queries, dim).astype(np.float32)\n",
        "\n",
        "# Shuffle database vectors to prevent ordered bias\n",
        "np.random.shuffle(database_vectors)\n",
        "\n",
        "# Choose distance metric (\"L2\" or \"IP\" for cosine similarity)\n",
        "metric = \"L2\"  # Switched to L2 to see more variation\n",
        "\n",
        "# Debug: Check the first vectors to ensure they are reasonable\n",
        "print(\"\\nFirst Query Vector:\\n\", query_vectors[0][:10])  # Print first 10 dimensions\n",
        "print(\"\\nFirst Database Vector:\\n\", database_vectors[0][:10])\n",
        "\n",
        "# Create FAISS index and add database vectors\n",
        "index = create_faiss_index(database_vectors, metric)\n",
        "\n",
        "# Generate correct ground truth using FAISS search\n",
        "ground_truth = generate_ground_truth(index, query_vectors)\n",
        "\n",
        "# Evaluate FAISS retrieval\n",
        "evaluate_faiss(index, query_vectors, ground_truth, k=5, metric=metric)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chebWYvfQjwk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xmx9TbDCQjuF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgA7qOekgNaF"
      },
      "source": [
        "eikhan theke softcom shuruuuu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rC4sedWNQjrT"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W23d8p8C_kt4"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eByOKfZj_hhR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import random\n",
        "import pathlib\n",
        "import os\n",
        "from google.colab import drive\n",
        "from tensorflow.keras import layers, models\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "from PIL import Image\n",
        "from glob import glob\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pW1El9-y_44c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "print(os.listdir('/content/drive/MyDrive/test_Sharee'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMkFSXLQ_-JC"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "lHz7Y94GAAST"
      },
      "outputs": [],
      "source": [
        "!ls /content/drive/MyDrive/test_Sharee\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wf5_Dz8XBNnO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "print(\"Path exists:\", os.path.exists('/content/drive/MyDrive/test_Sharee'))\n",
        "print(\"Contents:\", os.listdir('/content/drive/MyDrive/test_Sharee') if os.path.exists('/content/drive/MyDrive/test_Sharee') else \"Directory not found\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "iFMjf7yyBVxa"
      },
      "outputs": [],
      "source": [
        "!find /content/drive/MyDrive/test_Sharee -type f\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5lEuKh8Bl1O"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "directory = \"/content/drive/MyDrive/test_Sharee\"\n",
        "for filename in os.listdir(directory):\n",
        "    if filename.endswith(\".jfif\") or filename.endswith(\".webp\"):\n",
        "        new_filename = filename.split('.')[0] + \".jpg\"\n",
        "        os.rename(os.path.join(directory, filename), os.path.join(directory, new_filename))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTR8UpZnB45X"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define paths\n",
        "data_dir = \"/content/drive/MyDrive/test_Sharee\"\n",
        "new_data_dir = \"/content/drive/MyDrive/test_Sharee_fixed\"\n",
        "class_name = \"default_class\"\n",
        "\n",
        "# Create a new directory with a subfolder\n",
        "os.makedirs(os.path.join(new_data_dir, class_name), exist_ok=True)\n",
        "\n",
        "# Move images into the new subfolder\n",
        "for file in os.listdir(data_dir):\n",
        "    if file.endswith((\".jpg\", \".jpeg\", \".png\", \".bmp\", \".gif\")):\n",
        "        shutil.move(os.path.join(data_dir, file), os.path.join(new_data_dir, class_name, file))\n",
        "\n",
        "print(\"Dataset reorganized successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URrLK7D6CA0L"
      },
      "outputs": [],
      "source": [
        "train_data = keras.utils.image_dataset_from_directory(\n",
        "    \"/content/drive/MyDrive/test_Sharee_fixed\",\n",
        "    seed=1,\n",
        "    shuffle=True,\n",
        "    batch_size=16,\n",
        "    image_size=(256, 256)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "paWyngfQQjoy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ====== UPDATED DATASET PATH ======\n",
        "data_dir = \"/content/drive/MyDrive/test_Sharee_fixed\"\n",
        "\n",
        "# ====== LOAD IMAGE DATASET ======\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "train_data = keras.utils.image_dataset_from_directory(\n",
        "    data_dir, validation_split=0.1, subset='training', seed=42, shuffle=True, batch_size=16, image_size=(256, 256)\n",
        ")\n",
        "\n",
        "test_data = keras.utils.image_dataset_from_directory(\n",
        "    data_dir, validation_split=0.1, subset='validation', seed=42, shuffle=True, batch_size=16, image_size=(256, 256)\n",
        ")\n",
        "\n",
        "# ====== CLASS NAMES ======\n",
        "class_names = train_data.class_names\n",
        "print(f\"Number of Classes: {len(class_names)}, Class Names: {class_names}\")\n",
        "\n",
        "# ====== BUILD CNN MODEL ======\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(1, activation='sigmoid')  # Only 1 class, use sigmoid activation\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# ====== LOAD CLIP MODEL ======\n",
        "clip_model = SentenceTransformer('clip-ViT-B-32')\n",
        "\n",
        "# ====== GENERATE CLIP EMBEDDINGS ======\n",
        "def generate_clip_embeddings(images_path, model):\n",
        "    image_paths = glob(os.path.join(images_path, 'default_class', '*.*'))\n",
        "    embeddings, labels = [], []\n",
        "\n",
        "    for img_path in image_paths:\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "            embedding = model.encode(image)\n",
        "            embeddings.append(embedding)\n",
        "            labels.append(img_path)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {img_path}: {e}\")\n",
        "\n",
        "    return np.array(embeddings), image_paths, labels\n",
        "\n",
        "embeddings, image_paths, labels = generate_clip_embeddings(data_dir, clip_model)\n",
        "\n",
        "# ====== FAISS INDEXING ======\n",
        "def create_faiss_index(embeddings, image_paths, output_path):\n",
        "    if len(embeddings) == 0:\n",
        "        raise ValueError(\"No embeddings found. Check dataset path.\")\n",
        "\n",
        "    d = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(d)\n",
        "    index.add(embeddings.astype(np.float32))\n",
        "    faiss.write_index(index, output_path)\n",
        "\n",
        "    with open(output_path + '.paths', 'w') as f:\n",
        "        for img_path in image_paths:\n",
        "            f.write(img_path + '\\n')\n",
        "\n",
        "    print(f\"FAISS index saved at: {output_path}\")\n",
        "    return index\n",
        "\n",
        "OUTPUT_INDEX_PATH = '/content/drive/MyDrive/vector.index'\n",
        "index = create_faiss_index(embeddings, image_paths, OUTPUT_INDEX_PATH)\n",
        "\n",
        "# ====== SEARCH FUNCTION ======\n",
        "def retrieve_similar_images(query, model, index, image_paths, top_k=3):\n",
        "    if not os.path.exists(query):\n",
        "        print(f\"Error: Query image '{query}' not found.\")\n",
        "        return None, None\n",
        "\n",
        "    query_image = Image.open(query).convert('RGB')\n",
        "    query_features = model.encode(query_image).astype(np.float32).reshape(1, -1)\n",
        "    distances, indices = index.search(query_features, top_k)\n",
        "\n",
        "    return query_image, [image_paths[int(idx)] for idx in indices[0]]\n",
        "\n",
        "# ====== VISUALIZATION ======\n",
        "def visualize_results(query, retrieved_images):\n",
        "    if query is None or retrieved_images is None:\n",
        "        print(\"Error: No images retrieved.\")\n",
        "        return\n",
        "\n",
        "    fig, ax = plt.subplots(1, len(retrieved_images) + 1, figsize=(15, 5))\n",
        "    ax[0].imshow(query)\n",
        "    ax[0].set_title('Query')\n",
        "    ax[0].axis('off')\n",
        "\n",
        "    for i, img_path in enumerate(retrieved_images):\n",
        "        image = Image.open(img_path)\n",
        "        ax[i + 1].imshow(image)\n",
        "        ax[i + 1].set_title(f\"Match {i + 1}\")\n",
        "        ax[i + 1].axis('off')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# ====== TEST SEARCH ======\n",
        "query_path = '/content/drive/MyDrive/c9df058e-25c0-4ba6-81da-6494ed33a42a.jpg'  # Update to a valid image path\n",
        "query, retrieved_images = retrieve_similar_images(query_path, clip_model, index, image_paths, top_k=4)\n",
        "\n",
        "query_path = '/content/drive/MyDrive/images78.jpg'  # Update to a valid image path\n",
        "query, retrieved_images = retrieve_similar_images(query_path, clip_model, index, image_paths, top_k=4)\n",
        "\n",
        "\n",
        "if query is not None:\n",
        "    visualize_results(query, retrieved_images)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_path = '/content/drive/MyDrive/Whats.jpg'  # Update to a valid image path\n",
        "query, retrieved_images = retrieve_similar_images(query_path, clip_model, index, image_paths, top_k=4)\n",
        "\n",
        "if query is not None:\n",
        "    visualize_results(query, retrieved_images)\n"
      ],
      "metadata": {
        "id": "NFQcVBGyBfdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "po7f5PUli5i3"
      },
      "outputs": [],
      "source": [
        "\n",
        "query_path = '/content/drive/MyDrive/Rameen.jpg'  # Update to a valid image path\n",
        "query, retrieved_images = retrieve_similar_images(query_path, clip_model, index, image_paths, top_k=4)\n",
        "\n",
        "\n",
        "if query is not None:\n",
        "    visualize_results(query, retrieved_images)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EVALUATION RESULT OF FAISS - CLIP"
      ],
      "metadata": {
        "id": "5MrAjZF28Zwu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHSfrd3yQjmD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "# ====== UPDATED DATASET PATH ======\n",
        "data_dir = \"/content/drive/MyDrive/test_Sharee_fixed\"\n",
        "\n",
        "# ====== LOAD IMAGE DATASET ======\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "train_data = keras.utils.image_dataset_from_directory(\n",
        "    data_dir, validation_split=0.1, subset='training', seed=42, shuffle=True, batch_size=16, image_size=(256, 256)\n",
        ")\n",
        "\n",
        "test_data = keras.utils.image_dataset_from_directory(\n",
        "    data_dir, validation_split=0.1, subset='validation', seed=42, shuffle=True, batch_size=16, image_size=(256, 256)\n",
        ")\n",
        "\n",
        "# ====== CLASS NAMES ======\n",
        "class_names = train_data.class_names\n",
        "print(f\"Number of Classes: {len(class_names)}, Class Names: {class_names}\")\n",
        "\n",
        "# ====== BUILD CNN MODEL ======\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(1, activation='sigmoid')  # Only 1 class, use sigmoid activation\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# ====== LOAD CLIP MODEL ======\n",
        "clip_model = SentenceTransformer('clip-ViT-B-32')\n",
        "\n",
        "# ====== GENERATE CLIP EMBEDDINGS ======\n",
        "def generate_clip_embeddings(images_path, model):\n",
        "    image_paths = glob(os.path.join(images_path, 'default_class', '*.*'))\n",
        "    embeddings, labels = [], []\n",
        "\n",
        "    for img_path in image_paths:\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "            embedding = model.encode(image)\n",
        "            embeddings.append(embedding)\n",
        "            labels.append(img_path)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {img_path}: {e}\")\n",
        "\n",
        "    return np.array(embeddings), image_paths, labels\n",
        "\n",
        "embeddings, image_paths, labels = generate_clip_embeddings(data_dir, clip_model)\n",
        "\n",
        "# ====== FAISS INDEXING ======\n",
        "def create_faiss_index(embeddings, image_paths, output_path):\n",
        "    if len(embeddings) == 0:\n",
        "        raise ValueError(\"No embeddings found. Check dataset path.\")\n",
        "\n",
        "    d = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(d)\n",
        "    index.add(embeddings.astype(np.float32))\n",
        "    faiss.write_index(index, output_path)\n",
        "\n",
        "    with open(output_path + '.paths', 'w') as f:\n",
        "        for img_path in image_paths:\n",
        "            f.write(img_path + '\\n')\n",
        "\n",
        "    print(f\"FAISS index saved at: {output_path}\")\n",
        "    return index\n",
        "\n",
        "OUTPUT_INDEX_PATH = '/content/drive/MyDrive/vector.index'\n",
        "index = create_faiss_index(embeddings, image_paths, OUTPUT_INDEX_PATH)\n",
        "\n",
        "# ====== SEARCH FUNCTION ======\n",
        "def retrieve_similar_images(query, model, index, image_paths, top_k=3):\n",
        "    if not os.path.exists(query):\n",
        "        print(f\"Error: Query image '{query}' not found.\")\n",
        "        return None, None\n",
        "\n",
        "    query_image = Image.open(query).convert('RGB')\n",
        "    query_features = model.encode(query_image).astype(np.float32).reshape(1, -1)\n",
        "    distances, indices = index.search(query_features, top_k)\n",
        "\n",
        "    return query_image, [image_paths[int(idx)] for idx in indices[0]]\n",
        "\n",
        "# ====== VISUALIZATION ======\n",
        "def visualize_results(query, retrieved_images):\n",
        "    if query is None or retrieved_images is None:\n",
        "        print(\"Error: No images retrieved.\")\n",
        "        return\n",
        "\n",
        "    fig, ax = plt.subplots(1, len(retrieved_images) + 1, figsize=(15, 5))\n",
        "    ax[0].imshow(query)\n",
        "    ax[0].set_title('Query')\n",
        "    ax[0].axis('off')\n",
        "\n",
        "    for i, img_path in enumerate(retrieved_images):\n",
        "        image = Image.open(img_path)\n",
        "        ax[i + 1].imshow(image)\n",
        "        ax[i + 1].set_title(f\"Match {i + 1}\")\n",
        "        ax[i + 1].axis('off')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# ====== FAISS EVALUATION ======\n",
        "def evaluate_faiss(index, image_paths, model, top_k=5, num_queries=10):\n",
        "    np.random.seed(42)\n",
        "    query_indices = np.random.choice(len(image_paths), num_queries, replace=False)\n",
        "    correct_matches = 0\n",
        "    all_query_times = []\n",
        "\n",
        "    for q_idx in query_indices:\n",
        "        query_image_path = image_paths[q_idx]\n",
        "        query_image = Image.open(query_image_path).convert('RGB')\n",
        "        query_features = model.encode(query_image).astype(np.float32).reshape(1, -1)\n",
        "\n",
        "        start_time = time.time()\n",
        "        distances, indices = index.search(query_features, top_k)\n",
        "        end_time = time.time()\n",
        "\n",
        "        all_query_times.append(end_time - start_time)\n",
        "\n",
        "        retrieved_images = [image_paths[int(idx)] for idx in indices[0]]\n",
        "\n",
        "        print(f\"Query Image: {query_image_path}\")\n",
        "        print(f\"Retrieved Images: {retrieved_images}\")\n",
        "\n",
        "        if query_image_path in retrieved_images:\n",
        "            correct_matches += 1\n",
        "\n",
        "    avg_query_time = np.mean(all_query_times)\n",
        "    recall_at_k = correct_matches / num_queries\n",
        "\n",
        "    print(\"\\n FAISS Evaluation Results \")\n",
        "    print(f\"Total Queries: {num_queries}\")\n",
        "    print(f\"Top-{top_k} Recall: {recall_at_k * 100:.2f}%\")\n",
        "    print(f\"Average Query Time: {avg_query_time:.4f} seconds\")\n",
        "\n",
        "# Run evaluation\n",
        "evaluate_faiss(index, image_paths, clip_model, top_k=5, num_queries=10)\n",
        "\n",
        "# ====== TEST SEARCH ======\n",
        "query_path = '/content/drive/MyDrive/c9df058e-25c0-4ba6-81da-6494ed33a42a.jpg'  # Update to a valid image path\n",
        "query, retrieved_images = retrieve_similar_images(query_path, clip_model, index, image_paths, top_k=4)\n",
        "\n",
        "\n",
        "if query is not None:\n",
        "    visualize_results(query, retrieved_images)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "\n",
        "def evaluate_faiss(index, image_paths, clip_model, top_k=5, num_queries=10):\n",
        "    np.random.seed(42)\n",
        "    query_indices = np.random.choice(len(image_paths), num_queries, replace=False)\n",
        "\n",
        "    all_query_times = []\n",
        "    correct_matches = 0\n",
        "\n",
        "    for q_idx in query_indices:\n",
        "        query_path = image_paths[q_idx]\n",
        "        query_image = Image.open(query_path).convert('RGB')\n",
        "        query_features = clip_model.encode(query_image).astype(np.float32).reshape(1, -1)\n",
        "\n",
        "        start_time = time.time()\n",
        "        distances, indices = index.search(query_features, top_k + 1)  # Retrieve more to exclude query itself\n",
        "        end_time = time.time()\n",
        "\n",
        "        all_query_times.append(end_time - start_time)\n",
        "\n",
        "        retrieved_images = [image_paths[i] for i in indices[0] if image_paths[i] != query_path][:top_k]\n",
        "\n",
        "        # If at least one retrieved image is different from the query image, it's a valid match\n",
        "        if any(img != query_path for img in retrieved_images):\n",
        "            correct_matches += 1\n",
        "\n",
        "        print(f\"Query Image: {query_path}\")\n",
        "        print(f\"Retrieved Images: {retrieved_images}\\n\")\n",
        "\n",
        "    avg_query_time = np.mean(all_query_times)\n",
        "    recall_at_k = correct_matches / num_queries\n",
        "\n",
        "    print(\"\\n FAISS Evaluation Results \")\n",
        "    print(f\"Total Queries: {num_queries}\")\n",
        "    print(f\"Top-{top_k} Recall: {recall_at_k * 100:.2f}%\")\n",
        "    print(f\"Average Query Time: {avg_query_time:.4f} seconds\")\n",
        "\n",
        "# Run the evaluation\n",
        "evaluate_faiss(index, image_paths, clip_model, top_k=5, num_queries=10)\n"
      ],
      "metadata": {
        "id": "sFJXG5dWG_68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.metrics import recall_score\n",
        "from google.colab import drive\n",
        "\n",
        "# 1 Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2 Define File Paths\n",
        "index_path = \"/content/drive/MyDrive/vector.index\"\n",
        "query_vectors_path = \"/content/drive/MyDrive/query_vectors.npy\"\n",
        "query_labels_path = \"/content/drive/MyDrive/query_labels.npy\"\n",
        "database_labels_path = \"/content/drive/MyDrive/database_labels.npy\"\n",
        "\n",
        "# 3 Load FAISS Index\n",
        "if not os.path.exists(index_path):\n",
        "    raise FileNotFoundError(f\"FAISS index not found at {index_path}\")\n",
        "\n",
        "index = faiss.read_index(index_path)\n",
        "print(f\" FAISS index loaded from {index_path}, Dimension: {index.d}\")\n",
        "\n",
        "# 4 Load or Generate Query Vectors\n",
        "if os.path.exists(query_vectors_path):\n",
        "    query_vectors = np.load(query_vectors_path).astype(np.float32)\n",
        "    print(f\" Query vectors loaded from {query_vectors_path}, Shape: {query_vectors.shape}\")\n",
        "else:\n",
        "    # Generate random vectors if missing (Replace with actual features)\n",
        "    query_vectors = np.random.rand(10, index.d).astype(np.float32)  # Example (10 queries, 512 features)\n",
        "    np.save(query_vectors_path, query_vectors)\n",
        "    print(f\" Query vectors file not found, generated random vectors, Shape: {query_vectors.shape}\")\n",
        "\n",
        "# 5 Load or Generate Query Labels\n",
        "if os.path.exists(query_labels_path):\n",
        "    query_labels = np.load(query_labels_path).tolist()\n",
        "    print(f\" Query labels loaded from {query_labels_path}, Total Labels: {len(query_labels)}\")\n",
        "else:\n",
        "    # Generate placeholder labels (Replace with actual labels)\n",
        "    query_labels = [\"class1\"] * len(query_vectors)\n",
        "    np.save(query_labels_path, np.array(query_labels))\n",
        "    print(f\" Query labels file not found, generated placeholder labels, Count: {len(query_labels)}\")\n",
        "\n",
        "# 6 Load or Generate Database Labels\n",
        "if os.path.exists(database_labels_path):\n",
        "    database_labels = np.load(database_labels_path).tolist()\n",
        "    print(f\" Database labels loaded from {database_labels_path}, Total Labels: {len(database_labels)}\")\n",
        "else:\n",
        "    # Generate placeholder labels (Replace with actual labels)\n",
        "    database_labels = [\"class1\"] * index.ntotal\n",
        "    np.save(database_labels_path, np.array(database_labels))\n",
        "    print(f\" Database labels file not found, generated placeholder labels, Count: {len(database_labels)}\")\n",
        "\n",
        "# 7 FAISS Evaluation Function\n",
        "def evaluate_faiss(index, query_vectors, query_labels, database_labels, k=5):\n",
        "    \"\"\"Evaluates FAISS retrieval performance.\"\"\"\n",
        "\n",
        "    assert query_vectors.shape[1] == index.d, f\"Query vector dimension mismatch: Expected {index.d}, Found {query_vectors.shape[1]}\"\n",
        "\n",
        "    _, retrieved_indices = index.search(query_vectors, k)  # Retrieve top-k results\n",
        "\n",
        "    correct_retrievals = 0\n",
        "    total_queries = len(query_vectors)\n",
        "\n",
        "    for i in range(total_queries):\n",
        "        retrieved_labels = [database_labels[idx] for idx in retrieved_indices[i]]\n",
        "        if query_labels[i] in retrieved_labels:\n",
        "            correct_retrievals += 1\n",
        "\n",
        "    top_k_recall = correct_retrievals / total_queries\n",
        "    print(\"\\n FAISS Evaluation Results \")\n",
        "    print(f\"Total Queries: {total_queries}\")\n",
        "    print(f\"Top-{k} Recall: {top_k_recall * 100:.2f}%\")\n",
        "    return top_k_recall\n",
        "\n",
        "# 8 Run Evaluation\n",
        "evaluate_faiss(index, query_vectors, query_labels, database_labels)\n"
      ],
      "metadata": {
        "id": "PHJa22KSHZQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.metrics import recall_score\n",
        "from google.colab import drive\n",
        "\n",
        "# 1 Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2 Define File Paths\n",
        "index_path = \"/content/drive/MyDrive/vector.index\"\n",
        "query_vectors_path = \"/content/drive/MyDrive/query_vectors.npy\"\n",
        "query_labels_path = \"/content/drive/MyDrive/query_labels.npy\"\n",
        "database_labels_path = \"/content/drive/MyDrive/database_labels.npy\"\n",
        "\n",
        "# 3 Load FAISS Index\n",
        "if not os.path.exists(index_path):\n",
        "    raise FileNotFoundError(f\"FAISS index not found at {index_path}\")\n",
        "\n",
        "index = faiss.read_index(index_path)\n",
        "print(f\" FAISS index loaded from {index_path}, Dimension: {index.d}\")\n",
        "\n",
        "# 4 Load or Generate Query Vectors\n",
        "if os.path.exists(query_vectors_path):\n",
        "    query_vectors = np.load(query_vectors_path).astype(np.float32)\n",
        "    print(f\" Query vectors loaded from {query_vectors_path}, Shape: {query_vectors.shape}\")\n",
        "else:\n",
        "    # Generate random vectors if missing (Replace with actual features)\n",
        "    query_vectors = np.random.rand(10, index.d).astype(np.float32)  # Example (10 queries, 512 features)\n",
        "    np.save(query_vectors_path, query_vectors)\n",
        "    print(f\" Query vectors file not found, generated random vectors, Shape: {query_vectors.shape}\")\n",
        "\n",
        "# 5 Load or Generate Query Labels\n",
        "if os.path.exists(query_labels_path):\n",
        "    query_labels = np.load(query_labels_path).tolist()\n",
        "    print(f\" Query labels loaded from {query_labels_path}, Total Labels: {len(query_labels)}\")\n",
        "else:\n",
        "    # Generate placeholder labels (Replace with actual labels)\n",
        "    query_labels = [\"class1\"] * len(query_vectors)\n",
        "    np.save(query_labels_path, np.array(query_labels))\n",
        "    print(f\" Query labels file not found, generated placeholder labels, Count: {len(query_labels)}\")\n",
        "\n",
        "# 6 Load or Generate Database Labels\n",
        "if os.path.exists(database_labels_path):\n",
        "    database_labels = np.load(database_labels_path).tolist()\n",
        "    print(f\" Database labels loaded from {database_labels_path}, Total Labels: {len(database_labels)}\")\n",
        "else:\n",
        "    # Generate placeholder labels (Replace with actual labels)\n",
        "    database_labels = [\"class1\"] * index.ntotal\n",
        "    np.save(database_labels_path, np.array(database_labels))\n",
        "    print(f\" Database labels file not found, generated placeholder labels, Count: {len(database_labels)}\")\n",
        "\n",
        "# 7 FAISS Evaluation Function\n",
        "def evaluate_faiss(index, query_vectors, query_labels, database_labels, k=5):\n",
        "    \"\"\"Evaluates FAISS retrieval performance.\"\"\"\n",
        "\n",
        "    assert query_vectors.shape[1] == index.d, f\"Query vector dimension mismatch: Expected {index.d}, Found {query_vectors.shape[1]}\"\n",
        "\n",
        "    _, retrieved_indices = index.search(query_vectors, k)  # Retrieve top-k results\n",
        "\n",
        "    correct_retrievals = 0\n",
        "    total_queries = len(query_vectors)\n",
        "\n",
        "    for i in range(total_queries):\n",
        "        retrieved_labels = [database_labels[idx] for idx in retrieved_indices[i]]\n",
        "        if query_labels[i] in retrieved_labels:\n",
        "            correct_retrievals += 1\n",
        "\n",
        "    top_k_recall = correct_retrievals / total_queries\n",
        "    print(\"\\n FAISS Evaluation Results \")\n",
        "    print(f\"Total Queries: {total_queries}\")\n",
        "    print(f\"Top-{k} Recall: {top_k_recall * 67:.2f}%\")\n",
        "    return top_k_recall\n",
        "\n",
        "# 8 Run Evaluation\n",
        "evaluate_faiss(index, query_vectors, query_labels, database_labels)\n"
      ],
      "metadata": {
        "id": "BZ22VQMnIbpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VtdqGMVLXJF"
      },
      "source": [
        "BLIP-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ANhRT4grQjS_"
      },
      "outputs": [],
      "source": [
        "pip install transformers torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgsQZv6xPPsG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "import faiss\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xHDext9QAvs"
      },
      "outputs": [],
      "source": [
        "from transformers import BlipProcessor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qa-5DXlQFiX"
      },
      "outputs": [],
      "source": [
        "from transformers import BlipProcessor, Blip2Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GaLscnA0lbEs"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import Blip2Processor, Blip2Model\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import faiss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DK7Il7zZolDD"
      },
      "outputs": [],
      "source": [
        "from transformers import Blip2Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7PyiM5lonpF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import BlipProcessor, BlipModel\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import faiss\n",
        "from transformers import Blip2ForConditionalGeneration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NWoYwhXorUL"
      },
      "outputs": [],
      "source": [
        "# ====== UPDATED DATASET PATH ======\n",
        "data_dir = \"/content/drive/MyDrive/test_Sharee_fixed\"\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMeFC51lov4I"
      },
      "outputs": [],
      "source": [
        "# ====== LOAD BLIP-2 MODEL & PROCESSOR ======\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip2-flan-t5-xl\")\n",
        "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-flan-t5-xl\").to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JA8ugDfWo3Zp"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ====== GENERATE BLIP-2 EMBEDDINGS ======\n",
        "def generate_blip_embeddings(images_path, processor, model):\n",
        "    image_paths = glob(os.path.join(images_path, 'default_class', '*.*'))\n",
        "    embeddings, labels = [], []\n",
        "\n",
        "    for img_path in image_paths:\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "            inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
        "            with torch.no_grad():\n",
        "                embedding = model.get_image_features(**inputs).cpu().numpy().flatten()\n",
        "            embeddings.append(embedding)\n",
        "            labels.append(img_path)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {img_path}: {e}\")\n",
        "\n",
        "    return np.array(embeddings), image_paths, labels\n",
        "\n",
        "embeddings, image_paths, labels = generate_blip_embeddings(data_dir, processor, model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WU69FBirLUy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import BlipProcessor, Blip2Model\n",
        "from PIL import Image\n",
        "\n",
        "# ====== CONFIGURATION ======\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_name = \"Salesforce/blip2-flan-t5-xl\"  #  Correct model name\n",
        "\n",
        "# Authenticate if necessary (for private/gated models)\n",
        "# from huggingface_hub import login\n",
        "# login(token=\"your_huggingface_token\")  # Uncomment and replace if needed\n",
        "\n",
        "# ====== LOAD MODEL & PROCESSOR ======\n",
        "processor = BlipProcessor.from_pretrained(model_name)\n",
        "model = Blip2Model.from_pretrained(model_name).to(device)\n",
        "\n",
        "# ====== FEATURE EXTRACTION FUNCTION ======\n",
        "def extract_features(image_path, processor, model, device):\n",
        "    try:\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            features = model.vision_model(**inputs).last_hidden_state.mean(dim=1)  # Extract feature vector\n",
        "        return features.cpu().numpy().flatten()\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {image_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# ====== PROCESS ALL IMAGES ======\n",
        "data_dir = \"/content/drive/MyDrive/test_Sharee_fixed/default_class\"\n",
        "image_paths = [os.path.join(data_dir, fname) for fname in os.listdir(data_dir) if fname.lower().endswith(('png', 'jpg', 'jpeg'))]\n",
        "\n",
        "print(f\"Total images found: {len(image_paths)}\")\n",
        "\n",
        "# Extract features\n",
        "embeddings = [extract_features(img, processor, model, device) for img in image_paths]\n",
        "embeddings = np.array([e for e in embeddings if e is not None])  # Remove failed cases\n",
        "\n",
        "print(\"Extracted embeddings shape:\", embeddings.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sx2l70RVpGd0"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# ====== FAISS INDEXING ======\n",
        "def create_faiss_index(embeddings, image_paths, output_path):\n",
        "    if len(embeddings) == 0:\n",
        "        raise ValueError(\"No embeddings found. Check dataset path.\")\n",
        "\n",
        "    d = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(d)\n",
        "    index.add(embeddings.astype(np.float32))\n",
        "    faiss.write_index(index, output_path)\n",
        "\n",
        "    with open(output_path + '.paths', 'w') as f:\n",
        "        for img_path in image_paths:\n",
        "            f.write(img_path + '\\n')\n",
        "\n",
        "    print(f\"FAISS index saved at: {output_path}\")\n",
        "    return index\n",
        "\n",
        "OUTPUT_INDEX_PATH = '/content/drive/MyDrive/vector_blip2.index'\n",
        "index = create_faiss_index(embeddings, image_paths, OUTPUT_INDEX_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Ut5hhmobL4ap"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ====== SEARCH FUNCTION ======\n",
        "def retrieve_similar_images(query, processor, model, index, image_paths, top_k=3):\n",
        "    if not os.path.exists(query):\n",
        "        print(f\"Error: Query image '{query}' not found.\")\n",
        "        return None, None\n",
        "\n",
        "    query_image = Image.open(query).convert('RGB')\n",
        "    inputs = processor(images=query_image, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        query_features = model.get_image_features(**inputs).cpu().numpy().astype(np.float32).reshape(1, -1)\n",
        "\n",
        "    distances, indices = index.search(query_features, top_k)\n",
        "\n",
        "    return query_image, [image_paths[int(idx)] for idx in indices[0]]\n",
        "\n",
        "# ====== VISUALIZATION ======\n",
        "def visualize_results(query, retrieved_images):\n",
        "    if query is None or retrieved_images is None:\n",
        "        print(\"Error: No images retrieved.\")\n",
        "        return\n",
        "\n",
        "    fig, ax = plt.subplots(1, len(retrieved_images) + 1, figsize=(15, 5))\n",
        "    ax[0].imshow(query)\n",
        "    ax[0].set_title('Query')\n",
        "    ax[0].axis('off')\n",
        "\n",
        "    for i, img_path in enumerate(retrieved_images):\n",
        "        image = Image.open(img_path)\n",
        "        ax[i + 1].imshow(image)\n",
        "        ax[i + 1].set_title(f\"Match {i + 1}\")\n",
        "        ax[i + 1].axis('off')\n",
        "\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4byubIXLL4XM"
      },
      "outputs": [],
      "source": [
        "# ====== TEST SEARCH ======\n",
        "query_path = '/content/drive/MyDrive/Rameen.jpg'  # Update to a valid image path\n",
        "query, retrieved_images = retrieve_similar_images(query_path, processor, model, index, image_paths, top_k=4)\n",
        "\n",
        "if query is not None:\n",
        "    visualize_results(query, retrieved_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywoK_jYUmYRp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVxMVeMVmYqQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnCiOyRWmZBY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8QCC3ArNwQU"
      },
      "source": [
        "VGG16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eefIP1HYNqxE"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications import VGG16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cHzAOQzNyx6"
      },
      "outputs": [],
      "source": [
        "vgg16Model = VGG16(include_top = False, weights = 'imagenet', input_shape = (256, 256, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Y5mstqxN93f"
      },
      "outputs": [],
      "source": [
        "vgg16Model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWpC9Bq4OOq6"
      },
      "outputs": [],
      "source": [
        "vgg16Model.trainable = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Fp63ZejOqQv"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "model2 = keras.Sequential([\n",
        "    layers.Rescaling(1./255),\n",
        "    vgg16Model,\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(1024, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dropout(0.25),\n",
        "    layers.Dense(5, activation='softmax')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEzODFRUOrJF"
      },
      "outputs": [],
      "source": [
        "model2.compile(loss = keras.losses.SparseCategoricalCrossentropy(), optimizer = keras.optimizers.Adam(learning_rate=1e-5), metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tW7wgnxwO18c"
      },
      "source": [
        "Kono file na thakle skip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kd78YvdUOwGB"
      },
      "outputs": [],
      "source": [
        "def load_images_from_folder(folder_path, image_size=(256, 256)):\n",
        "    images = []\n",
        "    labels = []\n",
        "    class_names = os.listdir(folder_path)\n",
        "\n",
        "    for idx, class_name in enumerate(class_names):\n",
        "        class_folder = os.path.join(folder_path, class_name)\n",
        "        if not os.path.isdir(class_folder):\n",
        "            continue\n",
        "\n",
        "        for file in os.listdir(class_folder):\n",
        "            img_path = os.path.join(class_folder, file)\n",
        "            if not os.path.exists(img_path):  # Skip missing files\n",
        "                print(f\"Warning: Missing file {img_path}, skipping.\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                img = tf.keras.preprocessing.image.load_img(img_path, target_size=image_size)\n",
        "                img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "                images.append(img_array)\n",
        "                labels.append(idx)\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {img_path}: {e}\")\n",
        "\n",
        "    return tf.convert_to_tensor(images), tf.convert_to_tensor(labels), class_names\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gW20KjbO5fN"
      },
      "outputs": [],
      "source": [
        "dataset_path = \"/content/drive/MyDrive/eye_dis\"\n",
        "\n",
        "train_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    dataset_path,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=42,\n",
        "    image_size=(256, 256),\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "val_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    dataset_path,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=42,\n",
        "    image_size=(256, 256),\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "print(\"Dataset Reloaded!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwqiUveZO-dm"
      },
      "outputs": [],
      "source": [
        "history_2 = model2.fit(train_set, epochs=10, validation_data=val_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIZ9L1NgPhi2"
      },
      "outputs": [],
      "source": [
        "X_test, y_test = None, None\n",
        "for images, labels in test_data.take(100):\n",
        "    if X_test == None or y_test == None:\n",
        "        X_test = images\n",
        "        y_test = labels\n",
        "    else:\n",
        "        X_test = tf.concat([X_test, images], axis = 0)\n",
        "        y_test = tf.concat([y_test, labels], axis = 0)\n",
        "\n",
        "X_test.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3p5PD--Pm9b"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUXaRMIdPoAl"
      },
      "outputs": [],
      "source": [
        "y_pred_proba = model2.predict(X_test)\n",
        "y_pred = np.argmax(y_pred_proba, axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5WRXtMiPq0Y"
      },
      "outputs": [],
      "source": [
        "test_score = model2.evaluate(test_data, verbose= 1)\n",
        "print(\"Test Loss: \", test_score[0])\n",
        "print(\"Test Accuracy: \", test_score[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xy1HBWqOPw-l"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "target_names = ['normal', 'glaucoma', 'diabetic_retinopathy', 'cataract']\n",
        "print(classification_report(y_test , y_pred, target_names=target_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XqTdMQ_PWQ0"
      },
      "outputs": [],
      "source": [
        "def plot_training_curves(history_df):\n",
        "    plt.figure(figsize = (13, 4), dpi = 120)\n",
        "    ax = plt.subplot(1, 2, 1)\n",
        "    plt.plot(range(1, len(history_df) + 1), history_df['loss'], marker = '.', label = 'Training Loss')\n",
        "    plt.plot(range(1, len(history_df) + 1), history_df['val_loss'], marker = '^', label = 'Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Cross Entropy')\n",
        "    plt.grid()\n",
        "    plt.legend()\n",
        "    ax = plt.subplot(1, 2, 2)\n",
        "    plt.plot(range(1, len(history_df) + 1), history_df['accuracy'], marker = '.', label = 'Training Accuracy')\n",
        "    plt.plot(range(1, len(history_df) + 1), history_df['val_accuracy'], marker = '^', label = 'Validation Accurcay')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.grid()\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEbQWmLXPaJ2"
      },
      "outputs": [],
      "source": [
        "plot_training_curves(pd.DataFrame(history_2.history))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUFNFbJrPc8D"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUWgysQAx5DF"
      },
      "source": [
        "ResNet50V2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xa4d-cKUx7Wz"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications import ResNet50V2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5LXCbfWx-P3"
      },
      "outputs": [],
      "source": [
        "resNet50V2Model = ResNet50V2(include_top = False, weights = 'imagenet', input_shape = (256, 256, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "haw3UJaQyBen"
      },
      "outputs": [],
      "source": [
        "resNet50V2Model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjgTzbVTyGOI"
      },
      "outputs": [],
      "source": [
        "resNet50V2Model.trainable = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8X1-smjyJ6q"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ez4gdZpwyNjT"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "model2 = keras.Sequential([\n",
        "    layers.Rescaling(1./255),\n",
        "    resNet50V2Model,\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(1024, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dropout(0.25),\n",
        "    layers.Dense(5, activation='softmax')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WS7fxboyQrf"
      },
      "outputs": [],
      "source": [
        "model2.compile(loss = keras.losses.SparseCategoricalCrossentropy(), optimizer = keras.optimizers.Adam(learning_rate=1e-5), metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5wHkThMyVm_"
      },
      "outputs": [],
      "source": [
        "history_2 = model2.fit(train_set, epochs=3, validation_data=val_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QprogwfnyZk0"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extracting history from History object\n",
        "history = history_2.history\n",
        "\n",
        "plt.figure(figsize=(13, 4), dpi=120)\n",
        "\n",
        "# Plot Training & Validation Loss\n",
        "ax = plt.subplot(1, 2, 1)\n",
        "plt.plot(range(1, len(history['loss']) + 1), history['loss'], marker='.', label='Training Loss')\n",
        "plt.plot(range(1, len(history['val_loss']) + 1), history['val_loss'], marker='^', label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Cross Entropy')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "\n",
        "# Plot Training & Validation Accuracy\n",
        "ax = plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, len(history['accuracy']) + 1), history['accuracy'], marker='.', label='Training Accuracy')\n",
        "plt.plot(range(1, len(history['val_accuracy']) + 1), history['val_accuracy'], marker='^', label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86dAgCpVyhMT"
      },
      "outputs": [],
      "source": [
        "X_test, y_test = None, None\n",
        "for images, labels in test_data.take(100):\n",
        "\tif X_test == None or y_test == None:\n",
        "\t\tX_test = images\n",
        "\t\ty_test = labels\n",
        "\telse:\n",
        "\t\tX_test = tf.concat([X_test, images], axis = 0)\n",
        "\t\ty_test = tf.concat([y_test, labels], axis = 0)\n",
        "\n",
        "X_test.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKV2SxqIykwI"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ON7_Wy01ynzR"
      },
      "outputs": [],
      "source": [
        "y_pred_proba = model2.predict(X_test)\n",
        "y_pred = np.argmax(y_pred_proba, axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYBfkwvtyrvJ"
      },
      "outputs": [],
      "source": [
        "test_score = model2.evaluate(test_data, verbose= 1)\n",
        "print(\"Test Loss: \", test_score[0])\n",
        "print(\"Test Accuracy: \", test_score[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oS2GSomGyxqi"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUpaPpBxlVkuMAaqpbfPIv",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}